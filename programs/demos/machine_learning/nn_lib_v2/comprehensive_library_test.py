#!/usr/bin/env python3
"""
PMFlow BNN v0.2.0 Comprehensive Library Testing

This script thoroughly tests the library for logic and syntax errors that could cause failures.
Tests all modules, functions, and edge cases before deployment.
"""

import sys
import os
import traceback
import importlib
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Any

def test_module_imports():
    """Test all module imports for syntax errors."""
    print("üîç Testing Module Imports...")
    
    modules_to_test = [
        'pmflow_bnn',
        'pmflow_bnn.pmflow',
        'pmflow_bnn.bnn',
        'pmflow_bnn.factory',
        'pmflow_bnn.evaluation',
        'pmflow_bnn.utils',
        'pmflow_bnn.baselines',
        'pmflow_bnn.version'
    ]
    
    results = {}
    
    for module_name in modules_to_test:
        try:
            module = importlib.import_module(module_name)
            results[module_name] = "‚úÖ Success"
            print(f"   ‚úÖ {module_name}")
        except Exception as e:
            results[module_name] = f"‚ùå Error: {str(e)}"
            print(f"   ‚ùå {module_name}: {str(e)}")
    
    return results

def test_factory_functions():
    """Test factory functions with various configurations."""
    print("\nüè≠ Testing Factory Functions...")
    
    from pmflow_bnn import get_model_v2, get_performance_config
    
    test_cases = [
        {'model_type': 'temporal_pipeline'},
        {'model_type': 'always_plastic_v2'},
        {'model_type': 'standard_v2'},
        {'model_type': 'temporal_pipeline', 'n_centers': 16, 'pm_steps': 3},
        {'model_type': 'always_plastic_v2', 'plastic': True, 'plasticity_lr': 1e-3},
    ]
    
    if torch.cuda.device_count() > 1:
        test_cases.append({'model_type': 'multi_gpu'})
    
    results = {}
    
    for i, config in enumerate(test_cases):
        try:
            model = get_model_v2(**config)
            param_count = sum(p.numel() for p in model.parameters())
            results[f"config_{i+1}"] = f"‚úÖ Success: {param_count:,} params"
            print(f"   ‚úÖ Config {i+1}: {config['model_type']} - {param_count:,} params")
        except Exception as e:
            results[f"config_{i+1}"] = f"‚ùå Error: {str(e)}"
            print(f"   ‚ùå Config {i+1}: {str(e)}")
    
    # Test hardware configurations
    hardware_profiles = ['auto', 'cpu', 'single_gpu', 'jetson_nano']
    if torch.cuda.device_count() > 1:
        hardware_profiles.append('multi_gpu')
    
    print(f"\n   Testing hardware configurations:")
    for profile in hardware_profiles:
        try:
            config = get_performance_config(profile)
            results[f"hw_{profile}"] = f"‚úÖ Success: {config['model_type']}"
            print(f"     ‚úÖ {profile}: {config['model_type']}")
        except Exception as e:
            results[f"hw_{profile}"] = f"‚ùå Error: {str(e)}"
            print(f"     ‚ùå {profile}: {str(e)}")
    
    return results

def test_model_forward_passes():
    """Test forward passes with different inputs and configurations."""
    print("\nüß† Testing Model Forward Passes...")
    
    from pmflow_bnn import get_model_v2
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    test_configs = [
        {'model_type': 'temporal_pipeline', 'n_centers': 16, 'pm_steps': 3},
        {'model_type': 'standard_v2', 'n_centers': 12, 'pm_steps': 2},
    ]
    
    input_shapes = [
        (1, 28*28),     # Single sample
        (4, 28*28),     # Small batch
        (16, 28*28),    # Medium batch
        (32, 28*28),    # Large batch (if memory allows)
    ]
    
    results = {}
    
    for config_idx, config in enumerate(test_configs):
        print(f"   Testing {config['model_type']}...")
        
        try:
            model = get_model_v2(**config).to(device)
            model.eval()
            
            for shape in input_shapes:
                try:
                    with torch.no_grad():
                        x = torch.randn(shape, device=device)
                        
                        # Test standard forward pass
                        output = model(x)
                        if isinstance(output, tuple):
                            logits, hidden = output
                        else:
                            logits = output
                        
                        # Test with different temporal steps
                        if hasattr(model, 'forward') and 'T=' in str(model.forward.__code__.co_varnames):
                            output_t3 = model(x, T=3)
                            output_t5 = model(x, T=5)
                        
                        key = f"config_{config_idx+1}_shape_{shape[0]}"
                        results[key] = f"‚úÖ Success: {logits.shape}"
                        print(f"     ‚úÖ Batch {shape[0]}: Output shape {logits.shape}")
                        
                except Exception as e:
                    key = f"config_{config_idx+1}_shape_{shape[0]}"
                    results[key] = f"‚ùå Error: {str(e)}"
                    print(f"     ‚ùå Batch {shape[0]}: {str(e)}")
            
        except Exception as e:
            key = f"config_{config_idx+1}_model"
            results[key] = f"‚ùå Model creation failed: {str(e)}"
            print(f"   ‚ùå Model creation failed: {str(e)}")
    
    return results

def test_evaluation_framework():
    """Test the evaluation framework thoroughly."""
    print("\nüìä Testing Evaluation Framework...")
    
    from pmflow_bnn import PMFlowEvaluator, get_model_v2
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    results = {}
    
    try:
        # Create evaluator
        evaluator = PMFlowEvaluator(device=device)
        results['evaluator_creation'] = "‚úÖ Success"
        print("   ‚úÖ PMFlowEvaluator created")
        
        # Create test model
        model = get_model_v2('temporal_pipeline', n_centers=16, pm_steps=3).to(device)
        results['test_model'] = "‚úÖ Success"
        print("   ‚úÖ Test model created")
        
        # Generate test data
        test_data = torch.randn(100, 28*28)
        test_labels = torch.randint(0, 4, (100,))
        shifting_data = [
            (torch.randn(50, 28*28), torch.randint(0, 4, (50,))),
            (torch.randn(50, 28*28), torch.randint(0, 4, (50,))),
        ]
        results['test_data'] = "‚úÖ Success"
        print("   ‚úÖ Test data generated")
        
        # Test embarrassingly parallel scaling (small scale)
        try:
            ep_results = evaluator.evaluate_embarrassingly_parallel_scaling(
                model, max_batch_size=8, input_shape=(28*28,)
            )
            results['parallel_scaling'] = f"‚úÖ Success: {ep_results['peak_efficiency']:.2f}x peak"
            print(f"   ‚úÖ Parallel scaling: {ep_results['peak_efficiency']:.2f}x peak efficiency")
        except Exception as e:
            results['parallel_scaling'] = f"‚ùå Error: {str(e)}"
            print(f"   ‚ùå Parallel scaling: {str(e)}")
        
        # Test gravitational dynamics
        try:
            gd_results = evaluator.evaluate_gravitational_dynamics(
                model, test_data, test_labels, adaptation_steps=5
            )
            if gd_results:
                results['gravitational_dynamics'] = f"‚úÖ Success: {gd_results['mean_movement']:.4f} movement"
                print(f"   ‚úÖ Gravitational dynamics: {gd_results['mean_movement']:.4f} movement")
            else:
                results['gravitational_dynamics'] = "‚ö†Ô∏è No gravitational centers detected"
                print("   ‚ö†Ô∏è Gravitational dynamics: No centers detected")
        except Exception as e:
            results['gravitational_dynamics'] = f"‚ùå Error: {str(e)}"
            print(f"   ‚ùå Gravitational dynamics: {str(e)}")
        
        # Test biological plasticity (simplified)
        try:
            bp_results = evaluator.evaluate_biological_plasticity(
                model, test_data[:50], test_labels[:50], shifting_data
            )
            results['biological_plasticity'] = f"‚úÖ Success: {bp_results['plasticity_score']:.3f} score"
            print(f"   ‚úÖ Biological plasticity: {bp_results['plasticity_score']:.3f} score")
        except Exception as e:
            results['biological_plasticity'] = f"‚ùå Error: {str(e)}"
            print(f"   ‚ùå Biological plasticity: {str(e)}")
        
    except Exception as e:
        results['framework_error'] = f"‚ùå Framework error: {str(e)}"
        print(f"   ‚ùå Framework error: {str(e)}")
    
    return results

def test_edge_cases():
    """Test edge cases and error handling."""
    print("\n‚ö†Ô∏è  Testing Edge Cases...")
    
    from pmflow_bnn import get_model_v2, PMFlowEvaluator
    
    results = {}
    
    # Test invalid configurations
    invalid_configs = [
        {'model_type': 'nonexistent_model'},
        {'model_type': 'temporal_pipeline', 'n_centers': 0},
        {'model_type': 'temporal_pipeline', 'pm_steps': 0},
        {'model_type': 'temporal_pipeline', 'n_centers': -1},
    ]
    
    for i, config in enumerate(invalid_configs):
        try:
            model = get_model_v2(**config)
            results[f"invalid_{i+1}"] = "‚ùå Should have failed but didn't"
            print(f"   ‚ùå Invalid config {i+1} should have failed: {config}")
        except Exception as e:
            results[f"invalid_{i+1}"] = "‚úÖ Correctly rejected"
            print(f"   ‚úÖ Invalid config {i+1} correctly rejected: {str(e)[:50]}")
    
    # Test very small inputs
    try:
        model = get_model_v2('temporal_pipeline', n_centers=8, pm_steps=2)
        x = torch.randn(1, 28*28)
        output = model(x)
        results['small_input'] = "‚úÖ Success"
        print(f"   ‚úÖ Small input handled correctly")
    except Exception as e:
        results['small_input'] = f"‚ùå Error: {str(e)}"
        print(f"   ‚ùå Small input failed: {str(e)}")
    
    # Test memory constraints
    device = torch.device('cpu')  # Force CPU for memory test
    try:
        model = get_model_v2('temporal_pipeline', n_centers=64, pm_steps=5).to(device)
        x = torch.randn(64, 28*28, device=device)  # Large batch
        with torch.no_grad():
            output = model(x)
        results['memory_test'] = "‚úÖ Success"
        print(f"   ‚úÖ Memory constraints handled")
    except Exception as e:
        results['memory_test'] = f"‚ö†Ô∏è Memory issue: {str(e)[:50]}"
        print(f"   ‚ö†Ô∏è Memory test: {str(e)[:50]}")
    
    return results

def test_performance_benchmarks():
    """Test performance benchmarking capabilities."""
    print("\n‚ö° Testing Performance Benchmarks...")
    
    from pmflow_bnn import benchmark_temporal_parallelism, get_model_v2
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    results = {}
    
    try:
        model = get_model_v2('temporal_pipeline', n_centers=16, pm_steps=3).to(device)
        
        # Test benchmark function
        benchmark_results = benchmark_temporal_parallelism(
            model, batch_sizes=[2, 4, 8], device=device, num_trials=3
        )
        
        results['benchmark_function'] = f"‚úÖ Success: {len(benchmark_results['batch_sizes'])} sizes tested"
        print(f"   ‚úÖ Benchmark function: {len(benchmark_results['batch_sizes'])} batch sizes tested")
        
        # Check benchmark results structure
        required_keys = ['batch_sizes', 'forward_times', 'throughput', 'memory_usage']
        missing_keys = [key for key in required_keys if key not in benchmark_results]
        
        if not missing_keys:
            results['benchmark_structure'] = "‚úÖ All required keys present"
            print(f"   ‚úÖ Benchmark structure complete")
        else:
            results['benchmark_structure'] = f"‚ùå Missing keys: {missing_keys}"
            print(f"   ‚ùå Missing benchmark keys: {missing_keys}")
        
    except Exception as e:
        results['benchmark_error'] = f"‚ùå Error: {str(e)}"
        print(f"   ‚ùå Benchmark error: {str(e)}")
    
    return results

def generate_test_report(all_results: Dict[str, Dict]) -> str:
    """Generate comprehensive test report."""
    
    total_tests = sum(len(results) for results in all_results.values())
    passed_tests = sum(
        sum(1 for result in results.values() if result.startswith("‚úÖ"))
        for results in all_results.values()
    )
    
    report = f"""
PMFlow BNN v0.2.0 Library Test Report
=====================================

üìä SUMMARY:
   Total Tests: {total_tests}
   Passed: {passed_tests}
   Failed: {total_tests - passed_tests}
   Success Rate: {passed_tests/total_tests*100:.1f}%

"""
    
    for category, results in all_results.items():
        report += f"\nüìã {category.upper().replace('_', ' ')}:\n"
        for test, result in results.items():
            report += f"   {test}: {result}\n"
    
    # Overall assessment
    success_rate = passed_tests / total_tests
    if success_rate >= 0.9:
        report += f"\nüéâ OVERALL ASSESSMENT: EXCELLENT - Library ready for deployment!"
    elif success_rate >= 0.8:
        report += f"\n‚úÖ OVERALL ASSESSMENT: GOOD - Minor issues to address"
    elif success_rate >= 0.7:
        report += f"\n‚ö†Ô∏è OVERALL ASSESSMENT: MODERATE - Some issues need fixing"
    else:
        report += f"\n‚ùå OVERALL ASSESSMENT: POOR - Major issues require attention"
    
    return report

def main():
    """Run comprehensive library testing."""
    
    print("üß™ PMFlow BNN v0.2.0 Comprehensive Library Testing")
    print("="*55)
    print("Testing for logic/syntax errors before deployment...\n")
    
    all_results = {}
    
    # Run all test categories
    all_results['module_imports'] = test_module_imports()
    all_results['factory_functions'] = test_factory_functions()
    all_results['model_forward_passes'] = test_model_forward_passes()
    all_results['evaluation_framework'] = test_evaluation_framework()
    all_results['edge_cases'] = test_edge_cases()
    all_results['performance_benchmarks'] = test_performance_benchmarks()
    
    # Generate and display report
    report = generate_test_report(all_results)
    print(f"\n{report}")
    
    # Save report to file
    with open('library_test_report.txt', 'w') as f:
        f.write(report)
    
    print(f"\nüìÑ Test report saved to: library_test_report.txt")
    
    return all_results

if __name__ == "__main__":
    results = main()