{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5894d66c",
   "metadata": {},
   "source": [
    "# PMFlow v0.2.0 – CNN Baseline Evaluation Notebook\n",
    "\n",
    "This notebook mirrors the BNN testing flow but focuses on the CNN baselines provided in `nn_lib_v2/pmflow_bnn/baselines.py`. We'll build, train, evaluate, and optionally export a small CNN on MNIST, then compare with a simple MLP baseline. It also demonstrates using the existing PMFlowEvaluator where applicable (e.g., throughput/parallel scaling) even though CNNs don't expose gravitational centers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "719c7a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imported CNN/MLP baselines and evaluator from nn_lib_v2/pmflow_bnn\n"
     ]
    }
   ],
   "source": [
    "# 1) Import Dependencies\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local library path for nn_lib_v2 (adjust if needed)\n",
    "root = Path(__file__).resolve().parents[1] if '__file__' in globals() else Path.cwd().parents[0]\n",
    "lib_path = root / 'programs' / 'demos' / 'machine_learning' / 'nn_lib_v2'\n",
    "if lib_path.exists():\n",
    "    sys.path.insert(0, str(lib_path))\n",
    "\n",
    "try:\n",
    "    from pmflow_bnn.baselines import CNNBaseline, MLPBaseline\n",
    "    from pmflow_bnn.evaluation import PMFlowEvaluator\n",
    "    print(\"✅ Imported CNN/MLP baselines and evaluator from nn_lib_v2/pmflow_bnn\")\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Could not import from local nn_lib_v2. Error:\", e)\n",
    "    CNNBaseline = None\n",
    "    MLPBaseline = None\n",
    "    PMFlowEvaluator = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67316fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# 2) Configure Reproducibility and Device\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c7cca05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:03<00:00, 3.18MB/s]\n",
      "\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 126kB/s]\n",
      "\n",
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 959kB/s]\n",
      "\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 1.56MB/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test sizes: 55000/5000/10000\n"
     ]
    }
   ],
   "source": [
    "# 3) Load Dataset (MNIST)\n",
    "\n",
    "data_dir = str((Path.home() / '.torch' / 'datasets').resolve())\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # (1,28,28) in [0,1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "val_size = 5000\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_ds, val_ds = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "test_dataset = datasets.MNIST(root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=device.type=='cuda')\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=device.type=='cuda')\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=device.type=='cuda')\n",
    "\n",
    "print(f\"Train/Val/Test sizes: {len(train_ds)}/{len(val_ds)}/{len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f9853c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4) Inspect and Visualize Samples\n",
    "classes = [str(i) for i in range(10)]\n",
    "examples, labels = next(iter(train_loader))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(examples[i][0].numpy(), cmap='gray')\n",
    "    plt.title(f\"Label: {classes[labels[i].item()]}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc6650f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNBaseline(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Flatten(start_dim=1, end_dim=-1)\n",
      "    (9): Linear(in_features=6272, out_features=256, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "Trainable parameters: 1,701,130\n"
     ]
    }
   ],
   "source": [
    "# 5) Preprocess and Data Pipeline (already normalized via ToTensor)\n",
    "# Additional normalization to zero-mean/unit-variance could be added if desired.\n",
    "\n",
    "# 6) Define CNN Architecture (use library baseline)\n",
    "use_mlp_baseline = False  # flip to compare MLP\n",
    "\n",
    "num_classes = 10\n",
    "cnn = CNNBaseline(n_classes=num_classes).to(device) if CNNBaseline else None\n",
    "mlp = MLPBaseline(n_classes=num_classes).to(device) if (MLPBaseline and use_mlp_baseline) else None\n",
    "model = cnn if not use_mlp_baseline else mlp\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {param_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0054d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tmumford/Documents/gravity/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_acc=0.9929 val_acc=0.9878 train_loss=0.0235 val_loss=0.0368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | train_acc=0.9915 val_acc=0.9864 train_loss=0.0267 val_loss=0.0438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | train_acc=0.9956 val_acc=0.9898 train_loss=0.0138 val_loss=0.0349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | train_acc=0.9968 val_acc=0.9896 train_loss=0.0108 val_loss=0.0334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | train_acc=0.9989 val_acc=0.9924 train_loss=0.0041 val_loss=0.0229\n"
     ]
    }
   ],
   "source": [
    "# 7) Loss, Optimizer, Metrics\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total, correct, total_loss = 0, 0, 0.0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return correct/total, total_loss/total\n",
    "\n",
    "# 8) Train with simple early stopping (with progress bar + history)\n",
    "best_val_acc = 0.0\n",
    "patience, wait = 5, 0\n",
    "num_epochs = 5\n",
    "ckpt_path = str((Path.cwd() / 'cnn_mnist_ckpt.pt').resolve())\n",
    "\n",
    "from tqdm.auto import tqdm  # progress bar in notebooks/terminals\n",
    "\n",
    "# History for plots\n",
    "history = {\n",
    "    'epoch': [], 'train_acc': [], 'train_loss': [], 'val_acc': [], 'val_loss': [], 'epoch_sec': []\n",
    "}\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    start_t = time.time()\n",
    "    batch_iter = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\", leave=False)\n",
    "    for x, y in batch_iter:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        batch_iter.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    train_acc, train_loss = evaluate(train_loader)\n",
    "    val_acc, val_loss = evaluate(val_loader)\n",
    "    epoch_time = time.time() - start_t\n",
    "    history['epoch'].append(epoch)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['epoch_sec'].append(epoch_time)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | train_acc={train_acc:.4f} val_acc={val_acc:.4f} train_loss={train_loss:.4f} val_loss={val_loss:.4f} time={epoch_time:.1f}s\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        wait = 0\n",
    "        torch.save({'model_state': model.state_dict()}, ckpt_path)\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a261a30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy=0.9933, loss=0.0214\n"
     ]
    }
   ],
   "source": [
    "# 9) Evaluate on Test Set\n",
    "# Load best checkpoint\n",
    "if os.path.exists(ckpt_path):\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(state['model_state'])\n",
    "\n",
    "test_acc, test_loss = evaluate(test_loader)\n",
    "print(f\"Test accuracy={test_acc:.4f}, loss={test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee8b8f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded model batch preds: [7, 2, 1, 0, 4, 1, 4, 9, 5, 9]\n"
     ]
    }
   ],
   "source": [
    "# 10) Save and Load Checkpoints (explicit demo)\n",
    "save_demo_path = str((Path.cwd() / 'cnn_mnist_final.pt').resolve())\n",
    "torch.save({'model_state': model.state_dict()}, save_demo_path)\n",
    "\n",
    "# Reload and run a quick inference batch\n",
    "reloaded = CNNBaseline(n_classes=num_classes).to(device)\n",
    "reloaded.load_state_dict(torch.load(save_demo_path, map_location=device)['model_state'])\n",
    "reloaded.eval()\n",
    "\n",
    "x, y = next(iter(test_loader))\n",
    "x = x.to(device)\n",
    "with torch.no_grad():\n",
    "    logits = reloaded(x)\n",
    "    pred = logits.argmax(dim=1)\n",
    "print(\"Reloaded model batch preds:\", pred[:10].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce168e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-3 predictions for 4 images:\n",
      "  True=7 Preds=[7, 3, 9] Scores=[1.0, 0.0, 0.0]\n",
      "  True=2 Preds=[2, 1, 0] Scores=[1.0, 0.0, 0.0]\n",
      "  True=1 Preds=[1, 4, 7] Scores=[1.0, 0.0, 0.0]\n",
      "  True=0 Preds=[0, 6, 5] Scores=[1.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# 11) Run Inference on New Images (optional: synthetic example)\n",
    "@torch.no_grad()\n",
    "def infer_images(img_tensors, topk=3):\n",
    "    model.eval()\n",
    "    img_tensors = img_tensors.to(device)\n",
    "    logits = model(img_tensors)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    scores, idxs = probs.topk(topk, dim=1)\n",
    "    return scores.cpu().numpy(), idxs.cpu().numpy()\n",
    "\n",
    "# Take a few from test set\n",
    "imgs, labels = next(iter(test_loader))\n",
    "scores, idxs = infer_images(imgs[:4])\n",
    "print(\"Top-3 predictions for 4 images:\")\n",
    "for i in range(4):\n",
    "    print(f\"  True={labels[i].item()} Preds={idxs[i].tolist()} Scores={[round(s,3) for s in scores[i].tolist()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fec77e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation disabled. Set use_aug=True to enable.\n"
     ]
    }
   ],
   "source": [
    "# 12) Optional: Data Augmentation (switchable pipeline)\n",
    "use_aug = False\n",
    "if use_aug:\n",
    "    aug_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(28, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    aug_train = datasets.MNIST(root=data_dir, train=True, download=False, transform=aug_transform)\n",
    "    aug_loader = DataLoader(aug_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    # Demonstrate one quick epoch over aug data\n",
    "    model.train()\n",
    "    for x, y in aug_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Ran one augmentation epoch.\")\n",
    "else:\n",
    "    print(\"Augmentation disabled. Set use_aug=True to enable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc293d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sweep disabled. Set sweep=True to enable a quick LR sweep.\n"
     ]
    }
   ],
   "source": [
    "# 13) Optional: Hyperparameter Tuning Sweep (tiny demo)\n",
    "# For brevity, we'll sweep over two learning rates for 1 epoch each\n",
    "sweep = False\n",
    "if sweep:\n",
    "    lrs = [1e-3, 3e-4]\n",
    "    results = {}\n",
    "    for lr in lrs:\n",
    "        tmp_model = CNNBaseline(n_classes=num_classes).to(device)\n",
    "        tmp_opt = optim.Adam(tmp_model.parameters(), lr=lr)\n",
    "        tmp_crit = nn.CrossEntropyLoss()\n",
    "        tmp_model.train()\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            tmp_opt.zero_grad()\n",
    "            logits = tmp_model(x)\n",
    "            loss = tmp_crit(logits, y)\n",
    "            loss.backward()\n",
    "            tmp_opt.step()\n",
    "        acc, _ = evaluate(val_loader)\n",
    "        results[lr] = acc\n",
    "    print(\"Sweep results:\", results)\n",
    "else:\n",
    "    print(\"Sweep disabled. Set sweep=True to enable a quick LR sweep.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58b61b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX export disabled. Set export_onnx=True to enable.\n"
     ]
    }
   ],
   "source": [
    "# 14) Optional: Export Model to ONNX\n",
    "export_onnx = False\n",
    "onnx_path = str((Path.cwd() / 'cnn_mnist.onnx').resolve())\n",
    "if export_onnx:\n",
    "    dummy = torch.randn(1, 1, 28, 28, device=device)\n",
    "    torch.onnx.export(model, dummy, onnx_path, input_names=['input'], output_names=['logits'], opset_version=13)\n",
    "    print(\"Exported to:\", onnx_path)\n",
    "else:\n",
    "    print(\"ONNX export disabled. Set export_onnx=True to enable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02a2631b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic forward/backward test passed. Loss: 0.00038025982212275267\n"
     ]
    }
   ],
   "source": [
    "# 15) Optional: Basic Unit Tests for Data and Model\n",
    "assert next(iter(train_loader))[0].shape[1:] == (1, 28, 28), \"Expected MNIST tensors with shape (B, 1, 28, 28)\"\n",
    "\n",
    "# Quick forward-backward pass with a tiny batch for determinism\n",
    "_small_x, _small_y = next(iter(train_loader))\n",
    "_small_x, _small_y = _small_x[:8].to(device), _small_y[:8].to(device)\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "logits = model(_small_x)\n",
    "loss = criterion(logits, _small_y)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(\"Basic forward/backward test passed. Loss:\", float(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99717d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16) Learning curves: accuracy and loss over epochs\n",
    "if 'history' in globals() and len(history['epoch']) > 0:\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(16,4))\n",
    "    axs[0].plot(history['epoch'], history['train_acc'], label='train_acc')\n",
    "    axs[0].plot(history['epoch'], history['val_acc'], label='val_acc')\n",
    "    axs[0].set_title('Accuracy'); axs[0].set_xlabel('Epoch'); axs[0].legend(); axs[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axs[1].plot(history['epoch'], history['train_loss'], label='train_loss')\n",
    "    axs[1].plot(history['epoch'], history['val_loss'], label='val_loss')\n",
    "    axs[1].set_title('Loss'); axs[1].set_xlabel('Epoch'); axs[1].legend(); axs[1].grid(True, alpha=0.3)\n",
    "\n",
    "    axs[2].bar(history['epoch'], history['epoch_sec'])\n",
    "    axs[2].set_title('Epoch time (s)'); axs[2].set_xlabel('Epoch')\n",
    "    plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print('No training history found. Re-run training cell to populate history.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a5414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17) Confusion matrix and per-class accuracy\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "# Gather all predictions on test set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        all_preds.append(pred.cpu())\n",
    "        all_labels.append(y.cpu())\n",
    "\n",
    "all_preds = torch.cat(all_preds)\n",
    "all_labels = torch.cat(all_labels)\n",
    "num_classes = 10\n",
    "\n",
    "# Confusion matrix\n",
    "cm = torch.zeros((num_classes, num_classes), dtype=torch.int64)\n",
    "for t, p in zip(all_labels, all_preds):\n",
    "    cm[t, p] += 1\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "im = ax.imshow(cm.numpy(), cmap='Blues')\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_xticks(range(num_classes)); ax.set_yticks(range(num_classes))\n",
    "fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "ax.set_title('Confusion Matrix (counts)')\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "per_class_acc = []\n",
    "for c in range(num_classes):\n",
    "    total_c = cm[c].sum().item()\n",
    "    correct_c = cm[c, c].item()\n",
    "    per_class_acc.append(correct_c / total_c if total_c > 0 else 0.0)\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.bar(range(num_classes), per_class_acc)\n",
    "plt.xlabel('Class'); plt.ylabel('Accuracy')\n",
    "plt.title('Per-class Accuracy'); plt.ylim(0,1.0)\n",
    "plt.grid(True, axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print('Per-class accuracy:', [round(a,3) for a in per_class_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c70e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18) Misclassified examples gallery\n",
    "@torch.no_grad()\n",
    "def collect_misclassified(max_samples=36):\n",
    "    model.eval()\n",
    "    imgs, trues, preds, confs = [], [], [], []\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device)\n",
    "        logits = model(x)\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        pred = prob.argmax(dim=1).cpu()\n",
    "        conf = prob.max(dim=1).values.cpu()\n",
    "        y_cpu = y.cpu()\n",
    "        mis_mask = pred != y_cpu\n",
    "        if mis_mask.any():\n",
    "            idxs = mis_mask.nonzero(as_tuple=False).squeeze(1)\n",
    "            for i in idxs:\n",
    "                imgs.append(x[i].cpu())\n",
    "                trues.append(int(y_cpu[i]))\n",
    "                preds.append(int(pred[i]))\n",
    "                confs.append(float(conf[i]))\n",
    "                if len(imgs) >= max_samples:\n",
    "                    return imgs, trues, preds, confs\n",
    "    return imgs, trues, preds, confs\n",
    "\n",
    "mis_imgs, mis_trues, mis_preds, mis_confs = collect_misclassified()\n",
    "print(f\"Collected {len(mis_imgs)} misclassified samples.\")\n",
    "\n",
    "cols = 6\n",
    "rows = max(1, (len(mis_imgs) + cols - 1) // cols)\n",
    "plt.figure(figsize=(cols*2, rows*2))\n",
    "for i, img in enumerate(mis_imgs[:cols*rows]):\n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    plt.imshow(img[0].numpy(), cmap='gray')\n",
    "    plt.title(f\"t={mis_trues[i]} p={mis_preds[i]} ({mis_confs[i]:.2f})\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b49ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19) Optional: Throughput scaling on CNN (using PMFlowEvaluator-style plot)\n",
    "try:\n",
    "    from pmflow_bnn.evaluation import PMFlowEvaluator\n",
    "    evaluator = PMFlowEvaluator(device=device)\n",
    "\n",
    "    # Reuse evaluator's method but with CNN input shape (1,28,28)\n",
    "    # We'll wrap the CNN with a lambda to adapt the call signature.\n",
    "    class WrapCNN(torch.nn.Module):\n",
    "        def __init__(self, base):\n",
    "            super().__init__()\n",
    "            self.base = base\n",
    "        def forward(self, x):\n",
    "            # evaluator generates (B, 28*28); reshape\n",
    "            if x.ndim == 2 and x.shape[1] == 28*28:\n",
    "                x = x.view(x.size(0), 1, 28, 28)\n",
    "            return self.base(x)\n",
    "\n",
    "    wrapped = WrapCNN(model).to(device)\n",
    "    ep = evaluator.evaluate_embarrassingly_parallel_scaling(wrapped, max_batch_size=128, input_shape=(28*28,))\n",
    "\n",
    "    # Simple throughput plot\n",
    "    bs = ep['batch_sizes']\n",
    "    th = ep['throughputs']\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(bs, th, 'o-')\n",
    "    plt.xlabel('Batch size'); plt.ylabel('Samples/sec')\n",
    "    plt.title('CNN Throughput vs Batch Size')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print('Throughput scaling skipped:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0b5060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20) Final summary\n",
    "# Reuse existing test metrics and details computed above\n",
    "if 'test_acc' not in globals() or 'test_loss' not in globals():\n",
    "    test_acc, test_loss = evaluate(test_loader)\n",
    "\n",
    "best_cls = worst_cls = None\n",
    "if 'per_class_acc' in globals():\n",
    "    best_cls = int(np.argmax(per_class_acc))\n",
    "    worst_cls = int(np.argmin(per_class_acc))\n",
    "\n",
    "print('\\n===== SUMMARY =====')\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n",
    "print(f'Test loss:     {test_loss:.4f}')\n",
    "if best_cls is not None:\n",
    "    print(f'Best class:  {best_cls} (acc={per_class_acc[best_cls]:.3f})')\n",
    "    print(f'Worst class: {worst_cls} (acc={per_class_acc[worst_cls]:.3f})')\n",
    "if 'history' in globals() and len(history[\"epoch\"])>0:\n",
    "    print(f'Epochs run: {history[\"epoch\"][-1]} (avg {np.mean(history[\"epoch_sec\"]):.1f}s/epoch)')\n",
    "print('===================\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787b2206",
   "metadata": {},
   "source": [
    "## PMFlow core tests\n",
    "The following cells instantiate a PMFlow BNN from the library and run the meaningful capability evaluations (parallel scaling, gravitational dynamics, biological plasticity). This keeps the focus on PMFlow rather than CNN baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826b7d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) Instantiate PMFlow BNN with optimized config\n",
    "from pmflow_bnn import get_model_v2, get_performance_config\n",
    "\n",
    "pm_config = get_performance_config('auto')\n",
    "pm_config['model_type'] = 'always_plastic_v2'  # focus on PMFlow plasticity\n",
    "print('PMFlow config:', pm_config)\n",
    "\n",
    "pm_model = get_model_v2(**pm_config).to(device)\n",
    "pm_model.train()\n",
    "\n",
    "# Create simple MNIST-like tensors for evaluation\n",
    "train_data = torch.randn(200, 28*28)\n",
    "train_labels = torch.randint(0, 10, (200,))\n",
    "shifting_datasets = [\n",
    "    (torch.randn(120, 28*28), torch.randint(0, 10, (120,))),\n",
    "    (torch.randn(120, 28*28), torch.randint(0, 10, (120,)))\n",
    "]\n",
    "\n",
    "print('PMFlow model ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf937ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B) Run PMFlowEvaluator capability tests\n",
    "from pmflow_bnn import PMFlowEvaluator\n",
    "\n",
    "pm_evaluator = PMFlowEvaluator(device=device)\n",
    "\n",
    "# 1) Embarrassingly parallel scaling\n",
    "_ = pm_evaluator.evaluate_embarrassingly_parallel_scaling(pm_model, max_batch_size=64, input_shape=(28*28,))\n",
    "\n",
    "# 2) Gravitational dynamics (center movement/specialization)\n",
    "_ = pm_evaluator.evaluate_gravitational_dynamics(pm_model, train_data, train_labels, adaptation_steps=6)\n",
    "\n",
    "# 3) Biological plasticity (adaptation + retention)\n",
    "_ = pm_evaluator.evaluate_biological_plasticity(pm_model, train_data, train_labels, shifting_datasets)\n",
    "\n",
    "print('PMFlow capability tests complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0997d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C) Visualize and report PMFlow capabilities\n",
    "pm_evaluator.visualize_results()\n",
    "print(pm_evaluator.generate_report())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
